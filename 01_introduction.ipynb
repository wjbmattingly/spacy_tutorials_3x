{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-pathology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-click",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "antique-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./data/harry_potter.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-latitude",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "political-snapshot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "\n",
      "\n",
      "CHAPTER ONE\n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They\n"
     ]
    }
   ],
   "source": [
    "with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print (text[0:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-wagner",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-belarus",
   "metadata": {},
   "source": [
    "Here, we will begin to clean the data to prepare it properly for processing via spaCy. The first thing we will want to do is to separate the entire text into individual chapters. When trying to manipulate textual data in this way, it is always a good idea to look for patterns in the data that will easily allow you to manipulate it. In our case, the Harry Potter text begins each chapter with a capitalized \"CHAPTER\" followed by the number, spelled out. We can use this to split the entire text into chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "welcome-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE\n",
      "\n",
      "THE BOY WHO LIVED\n",
      "\n",
      "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say\n",
      "that they were perfectly normal, thank you very much. They were the last\n",
      "people you'd expect to be involved in anything strange or mysterious,\n",
      "because they just didn't hold with such nonsense.\n",
      "\n",
      "Mr. Dursley \n"
     ]
    }
   ],
   "source": [
    "chapters = text.split(\"CHAPTER \")[1:]\n",
    "print (chapters[0][0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-contamination",
   "metadata": {},
   "source": [
    "Now that we have each chapter separated, we can begin to break down the text further. We can tokenize it on the paragraph level and use those paragraphs as our basic size of the data that we will be passing to spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "actual-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for chapter in chapters:\n",
    "    paras = []\n",
    "    paragraphs = chapter.split(\"\\n\\n\")\n",
    "    for paragraph in paragraphs:\n",
    "        if paragraph != \"\":\n",
    "            paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "            paras.append(paragraph)\n",
    "    num = paras[0]\n",
    "    title = paras[1]\n",
    "    paras = paras[2:]\n",
    "    data.append((num, title, paras))\n",
    "\n",
    "with open (\"data/harry_potter_cleaned.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in data:\n",
    "        f.write(f\"{item[0]}: {item[1]}\")\n",
    "        for para in item[2]:\n",
    "            f.write(para+\"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-estate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-think",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-timber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-surname",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-fluid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-ecuador",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-cruise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-completion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
